{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline perceptron experiment\n",
    "In this notebook we implement a baseline one-layer neural network with _raw audio samples_ as an input and one _note probability vector_ as an output. After one epoch, the model reaches 30% accuracy on test set. Accuracy on train set is very low (23%) which suggests that the model capacity is too low.\n",
    "\n",
    "Metric | Result\n",
    "--- | ---\n",
    "Precision | 62.05%\n",
    "Recall | 37.33%\n",
    "Accuracy | 30.39%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for automatic reloading of my libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import datasets\n",
    "from model import Network\n",
    "\n",
    "import visualization as vis\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUSICNET_ROOT = \"/home/jirka/bakalarka/melody_extraction/datasets/musicnet\"\n",
    "test_data = datasets.musicnet_dataset(MUSICNET_ROOT, \"test\")\n",
    "train_data = datasets.musicnet_dataset(MUSICNET_ROOT, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_per_window = 1\n",
    "context_width = 944\n",
    "\n",
    "test_dataset = datasets.AADataset(test_data, annotations_per_window, context_width, shuffle_batches=False)\n",
    "train_dataset = datasets.AADataset(train_data, annotations_per_window, context_width, shuffle_batches=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small dataset for manual evaluation\n",
    "small_test_data = [\n",
    "    test_data[3].slice(15,20.8), # solo cello\n",
    "    test_data[9].slice(56,61.4), # solo piano\n",
    "    test_data[5].slice(55.6,61.6), # orchestra\n",
    "    test_data[2].slice(17.65,27), # violin + string section\n",
    "]\n",
    "small_test_dataset = datasets.AADataset(small_test_data, annotations_per_window, context_width, shuffle_batches=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(self, args):\n",
    "    # Perceptron baseline\n",
    "    audio_net = tf.layers.dense(self.window, 1000, activation=tf.nn.relu)\n",
    "    output_layer = tf.layers.dense(audio_net, self.note_range*self.annotations_per_window, activation=None, name=\"output\")\n",
    "    ref_notes_flat = tf.layers.flatten(self.ref_notes)\n",
    "\n",
    "    self.note_probabilites = tf.reshape(output_layer, [-1, self.annotations_per_window, self.note_range])\n",
    "    self.est_notes = tf.cast(tf.greater(self.note_probabilites, 0.5), tf.float32)\n",
    "    \n",
    "    self.loss = tf.losses.sigmoid_cross_entropy(ref_notes_flat, output_layer)\n",
    "\n",
    "    global_step = tf.train.create_global_step()\n",
    "    self.training = tf.train.AdamOptimizer().minimize(self.loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore = None\n",
    "\n",
    "# Create logdir name\n",
    "args = {\n",
    "    \"threads\": 6,\n",
    "    \"batch_size\": 64,\n",
    "    \"logdir\": None,\n",
    "    \"annotations_per_window\": test_dataset.annotations_per_window,\n",
    "    \"window_size\": test_dataset.window_size,\n",
    "    \"note_range\": 96,\n",
    "    \"samplerate\": test_dataset.samplerate\n",
    "}\n",
    "name = \"{}-bs{}-apw{}-wsiz{}\".format(\n",
    "    datetime.datetime.now().strftime(\"%m-%d_%H%M%S\"),\n",
    "    args[\"batch_size\"],\n",
    "    args[\"annotations_per_window\"],\n",
    "    args[\"window_size\"],\n",
    ")\n",
    "\n",
    "if restore:\n",
    "    name = restore\n",
    "args[\"logdir\"] = \"models/\" + name\n",
    "\n",
    "print(name)\n",
    "\n",
    "# Construct the network\n",
    "network = Network(threads=args[\"threads\"])\n",
    "network.construct(args, create_model)\n",
    "\n",
    "if restore:\n",
    "    network.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "network.train(train_dataset, test_dataset, small_test_dataset, args[\"batch_size\"], epochs, eval_every_n_batches=10000, save_every_n_batches=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.save()\n",
    "print(\"Testing dataset metrics:\")\n",
    "acc = network.evaluate(test_dataset, args[\"batch_size\"], print_detailed=True)\n",
    "print(\"\\nSmall testing dataset metrics:\")\n",
    "acc = network.evaluate(small_test_dataset, args[\"batch_size\"], visual_output=True, print_detailed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play the small testing dataset\n",
    "vis.samplesplayer(small_test_dataset.all_samples(), small_test_dataset.samplerate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
